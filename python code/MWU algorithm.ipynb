{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wD7Q-mvgISiT"
   },
   "source": [
    "This notebook provides an implemetation of the MWU Kirszbraun algrorithm as discribed in our paper [] (TBD).\n",
    "the algorthim evalutes a non parametric function $f:X \\rightarrow Y$ by a consistent lischitz function. for more details please read []. The algorithm works in two phases:\n",
    "1. Smoothing - where we evaluate the training points, based on seeing data X_train,Y_train, so they fit a smooth lipschitz funtion. The Lischitz constant is picked using a cross validation search on several candidates.\n",
    "2. Extension. Based on the smoothing, each new point can be evaluated in a way that the lipschitz constant is reserved. \n",
    "\n",
    "you can run this code on either \"cpu\" or \"gpu\".\n",
    "\n",
    "\n",
    "We start by import crucial packages, make sure you have them installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvStW8eSISie"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil,log,pi,inf,cos,sin,sqrt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from IPython.core.debugger import Tracer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csA68uZZISi0"
   },
   "source": [
    "## 1. Smoothing##\n",
    "Smoothing finds an evaluation of the X_train data, such that keep a fixed Lipschitz constant, that minimze the overall loss of the observed data Y_train.  $\\Phi(\\tilde{Y},Y) = \\frac{1}{n}\\sum_{i=1}^n ||y_i-\\tilde{y_i}||$.\n",
    "fixing the Lipshitz constant, allow us to prevent over fitting.\n",
    "The main algorithm is in SmoothMWU. FindSolutionAndReport and Smooth functions are wrapping of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zn6mVWOWISjz",
    "outputId": "5cd56581-13ee-44fc-e99d-34589f6e0a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "gpu_num = 0\n",
    "device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"{torch.cuda.get_device_name(gpu_num)}\")\n",
    "\n",
    "eps = 0.1 #epsilon - the precision\n",
    "\n",
    "\n",
    "# the running time can improved by a factor of 2\n",
    "# since we don't have to compute both |y_i - y_j| and |y_j - y_i|\n",
    "def PairwiseSqDistances(Y):\n",
    "    #Y is is an N-by-dimB matrix\n",
    "    [N, dimB] = list(Y.size())\n",
    "\n",
    "    repY  = Y.unsqueeze(1).expand(N, N, dimB)\n",
    "    repYt = repY.transpose(0, 1)\n",
    "    return torch.sum((repY - repYt) ** 2, 2)\n",
    "    #return torch.Tensor(euclidean_distances(Y)**2)\n",
    "    \n",
    "    \n",
    "def findLipschitzConstant(X,Y):\n",
    "    \"\"\"\n",
    "    given two sets of vectors of size n where f(X_i) = Y_i\n",
    "    finds the Lipschitz constant of f over (X,Y). \n",
    "    \"\"\"\n",
    "    \n",
    "    #return (torch.max(PairwiseSqDistances(Y) / (PairwiseSqDistances(X)+torch.eye(X.size()[0],device = device)))).to(device)\n",
    "    distY = torch.Tensor(euclidean_distances(Y,Y)**2)\n",
    "    distX = torch.Tensor(euclidean_distances(X,X)**2) \n",
    "    return  (torch.max(distY / (distX+torch.eye(distX.size()[0],device = device)))).to(device)\n",
    "    \n",
    "def SmoothMWU(Y, R_sq, Phi_0):\n",
    "    # Y is an N-by-dimB matrix; Y^T from the write-up\n",
    "    # R_sq is is an N-by-N matrix    \n",
    "#    from IPython.core.debugger import Tracer; Tracer()()\n",
    "    [N, dimB] = list(Y.size())\n",
    "    \n",
    "    m = N*(N-1) // 2 # the number of pair of points,\n",
    "    delta = eps / (2*m) #what is delta\n",
    "\n",
    "    # Initialize the weight matrix    \n",
    "    w_Phi = 0.5\n",
    "    W = torch.Tensor(N,N).to(device).fill_(0.5 / m)\n",
    "    for i in range(N):\n",
    "        R_sq[i,i] = 1 # why?\n",
    "\n",
    "    T = 350 # used for speed, for better resalts use sqrt(m)*log(N)/eps**2)\n",
    "\n",
    "    YSmoothed = torch.zeros(N, dimB, device = device) # returned output. initial guess\n",
    "\n",
    "    for t in range(T):\n",
    "        lambda_inv = Phi_0 / (w_Phi + eps/2) ## \\lambda^{-1}           \n",
    "\n",
    "        # set off-diagonal entries\n",
    "\n",
    "        L = -lambda_inv * torch.div(W + delta, R_sq)\n",
    "        \n",
    "        # set/fix the diagonal entries\n",
    "        S = torch.sum(L, dim = 1)\n",
    "        for i in range(N):\n",
    "            L[i,i] += -S[i] + 1\n",
    "\n",
    "        # solve for Yt ; gesv ==> solve in the next version of torch\n",
    "        Yt, _ = torch.gesv(Y, L)  \n",
    "        YSmoothed *= t/(t+1)\n",
    "        YSmoothed += Yt/(t+1)\n",
    "\n",
    "        # update the weights\n",
    "        # first update W, that is w_{ij}'s \n",
    "        PD = PairwiseSqDistances(Yt)\n",
    "        WUpdate = 1 + 2*eps * (torch.div(PD, R_sq) ** 0.5 - 1)\n",
    "        W *= WUpdate\n",
    "\n",
    "        # now update w_Phi\n",
    "        WPhiUpdate = 1 + 2*eps * ((torch.sum((Y - Yt) ** 2) / Phi_0) ** 0.5  - 1) \n",
    "        w_Phi *= WPhiUpdate        \n",
    "               \n",
    "        # renormalize\n",
    "        for i in range(N):\n",
    "            W[i,i] = 0\n",
    "\n",
    "        TotalW = torch.sum(W) / 2 + w_Phi\n",
    "        W /= TotalW\n",
    "        w_Phi /= TotalW\n",
    "\n",
    "    return YSmoothed\n",
    "        \n",
    "def Smooth(Y, R_sq):\n",
    "    [N, dimB] = list(Y.size())\n",
    "    YMean = (torch.sum(Y, dim = 1) / N).unsqueeze(1).expand(N, dimB)\n",
    "\n",
    "    Phi_0 = torch.sum((Y - YMean) ** 2) #a very crude upper bound\n",
    "    \n",
    "    PhiUB = Phi_0\n",
    "    PhiLB = 0\n",
    "    \n",
    "    while (PhiUB > (1 + eps/10) * PhiLB):\n",
    "        print(\"<\", end = \"\")\n",
    "        Phi_0 = (PhiLB + PhiUB) / 2\n",
    "        YSmoothed = SmoothMWU(Y, R_sq, Phi_0)\n",
    "        bLip = ( PairwiseSqDistances(YSmoothed) < (1 + eps) * R_sq ).all()\n",
    "        #bLip = ( PairwiseSqDistances(YSmoothed) < (1 + eps) * R_sq ).all()\n",
    "        bPhi = (torch.sum((Y - YSmoothed) ** 2) / Phi_0) < 1 + 2 * eps\n",
    "\n",
    "        if bLip and bPhi:\n",
    "            PhiUB = Phi_0\n",
    "        else:\n",
    "            PhiLB = Phi_0   \n",
    "        print(\">\", end = \"\", flush = True)         \n",
    " \n",
    "    print()\n",
    "    return SmoothMWU(Y, R_sq, PhiUB)\n",
    "\n",
    "def FindSolutionAndReport(Y, X,Lip, bReportVectors=True):\n",
    "    R_sq = PairwiseSqDistances(X)\n",
    "    #R_sq = torch.Tensor(euclidean_distances(X,X))\n",
    "    StartTime = time.time()\n",
    "    YSmoothed = Smooth(Y,Lip * R_sq)\n",
    "    phi = torch.mean((Y - YSmoothed) ** 2).item()\n",
    "    #d,n = X.size()[1]+Y.size()[1],X.size()[0] # important for risk bound using Rademacher complexity\n",
    "    #k = ( (d-1)*34*(4*Lip)**(d/2) ) / ( 2*sqrt(n) )\n",
    "    #rad = 8*k**(1/(d+1)) + d*k**((d-1)/(d+1)) - 2*(-(d+1)/2)\n",
    "    #rad_bound = phi+rad\n",
    "    print(\"Phi: \", phi)#, \" Rad:\", rad, \" Bound: \",rad_bound)\n",
    "    print(\"Lipschitz constant \",findLipschitzConstant(X,YSmoothed))\n",
    "    if bReportVectors:\n",
    "        print(\"vectors:\", YSmoothed)\n",
    "\n",
    "    print(\"Elapsed time\", round(time.time()- StartTime,2), \" sec\")\n",
    "    return YSmoothed #, rad_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_zSN-7iISk4",
    "scrolled": false
   },
   "source": [
    "## 2. Extension ##\n",
    "The extension evaluate a new point $x \\in X$ that preserved the lipschitz constant of the Z_train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1UUaRKnKISlB"
   },
   "outputs": [],
   "source": [
    "def nearestNeighbor(x,X,Y):\n",
    "#NEARESTNEIGHBOR finds the nearest neighbor and it's distance\n",
    "#   x - a vector\n",
    "#   X - group of vectors represented as row vectors.\n",
    "#   returns the nearest neigbor of x in X, the ditance ||x-nn||, and\n",
    "#   y0=f(x0)\n",
    "\n",
    "    N = X.size()[0]; # number of vectors in X\n",
    "    difference = torch.norm(X-x.repeat(N,1),dim=1)\n",
    "    dist,i = torch.min(difference),torch.argmin(difference)\n",
    "    nn = X[i,:]\n",
    "    y0 = Y[i,:]\n",
    "    return dist,y0\n",
    "\n",
    "def extension(x,X,Y,eps,L):\n",
    "    # extension finds a (1+eps) Lipshitz approximation for z=h(x)\n",
    "    #   x = the query point (vector)\n",
    "    #   X = the training set samples\n",
    "    #   Y = the training set labeling (not the true lables). actualy Y=Z\n",
    "    #   eps = the approximation parameter. in (0,1/2)\n",
    "    #   z = the output , the extension of h to x. z=h(x) s.t the lipshitz\n",
    "    #   constnant is less then (1+eps)L\n",
    "    ## the marks numbers are for being consistent with the paper of the work.\n",
    "    \n",
    "    # basic parameters\n",
    "    n = X.size()[0]; # sample size\n",
    "    DOF2 = Y.size()[1] # dimension of Y \n",
    "    \n",
    "    #1. find nearest neighbour of x out of X; y0 = f(x0) ; d0 = ||x0-x||\n",
    "    d0,y0 = nearestNeighbor(x,X,Y)\n",
    "    #2. T is nuber of iterations\n",
    "    T = 1000 # for better results use torch.min(ceil(16*log(n)/eps**2)\n",
    "    #3. initialize weights vector w where w1=1/n for each i\n",
    "    w = torch.ones(n,T+1).to(device)*1/n;\n",
    "    #4. initialize distance vector d where di = ||xi - x||\n",
    "    d = L*torch.norm(X-x.repeat(n,1),dim=1)\n",
    "    #5. Steps 6-10 will be repeated until convergence\n",
    "    for t in range(T):\n",
    "        #6. create a distribution\n",
    "        P = torch.sum(w[:,t]/(d**2)); #normaplization parameter\n",
    "        p = w[:,t]/(P*(d**2));\n",
    "        #7. z0 = sum(pi*yi) and delta = ||z0-y0||\n",
    "        z0 = torch.sum(Y*p.repeat(DOF2,1).transpose(0,1),dim=0)\n",
    "        delta = torch.norm(z0-y0);\n",
    "        #8. evalute z\n",
    "        if delta <= d0:\n",
    "            z = z0\n",
    "        else:\n",
    "            z = d0/delta*z0+(delta-d0)/delta*y0\n",
    "        #9. update weights : wi(t+1) = (1+(eps*||z-yi||/8di))*wi(t) for all i \n",
    "        tmp_dist = torch.norm(z.repeat(n,1)-Y,dim=1)\n",
    "        w[:,t+1]=torch.ones(n,device=device)+eps*tmp_dist/(8*d)*w[:,t]        \n",
    "        #10. normalize the weitghts\n",
    "        W = torch.sum(w[:,t+1]);\n",
    "        w[:,t+1] = (1/W)*w[:,t+1];\n",
    "    #11. average over weights\n",
    "    final_w = torch.sum(w,dim=1)*1/(T+1);\n",
    "    #12. compute z as in 6-8\n",
    "    #6. \n",
    "    P = torch.sum(final_w*(d**2)); #normaplization parameter\n",
    "    p = final_w/(P*(d**2));\n",
    "    #7. z0 = sum(pi*yi) and delta = ||z0-y0||\n",
    "    z0 = torch.sum(Y*p.repeat(DOF2,1).transpose(0,1),dim=0)\n",
    "    delta = torch.norm(z0-y0);\n",
    "    #8. evalute z\n",
    "    if delta <= d0:\n",
    "        z = z0\n",
    "    else:\n",
    "        z = d0/delta*z0+(delta-d0)/delta*y0\n",
    "    return z\n",
    "\n",
    "def Test(X_val,X_train,Z_train,eps,l):\n",
    "    n_val = X_val.size()[0] # validation/test set size\n",
    "    DOF2 = Z_train.size()[1];    # dimension of Y, the second agent\n",
    "    Z_val = torch.ones(n_val,DOF2).to(device)\n",
    "    for i in range(n_val):\n",
    "        x0 = X_val[i]\n",
    "        Z_val[i] = extension(x0,X_train,Z_train,eps,l)\n",
    "#debug        print(Z_val[i])\n",
    "    return Z_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_HTVvOZISlU"
   },
   "source": [
    "## 3. Train and Test\n",
    "the next section is for using the algorithm in order to train and test the algorithm.\n",
    "The training consist of finding the optimal Lipshchitz $l$ constant using cross validation over different $l$ candidates.\n",
    "We also used the Structur Risk Minimization (SRM) which finds the $l$ which yields to the smallest generalization bound, but in practice this factor is non informative unless n is on a very large scale (milions). \n",
    "It is likely the SRM will be deleted in future for now you can uncomment it for your own use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDLHKG5HISlZ"
   },
   "outputs": [],
   "source": [
    "def lstsqrs(Z,Y):\n",
    "    ### will return the distance phi = sum(||Z-Y||**2)\n",
    "    ### when dealing with angels we notice that distance(0,2pi) = 0\n",
    "    N = Z.size()[0]\n",
    "    return torch.sum(torch.norm(Z-Y,dim=1)**2)/N\n",
    "\n",
    "\n",
    "#mod_lstqr consider the periodic of distance between angles\n",
    "def mod_lstsqr(Z,Y):\n",
    "    ### will return the distance phi = sum(||Z-Y||**2)\n",
    "    ### when dealing with angels we notice that distance(0,2pi) = 0\n",
    "    diff = torch.min((Z-Y)%(2*pi),(Y-Z)%(2*pi))\n",
    "    dist = torch.norm(diff,dim=1)**2\n",
    "    return dist.mean()\n",
    "    \n",
    "def crossVal(X,Y,X_val,Y_val,k_fold = 10):\n",
    "    #basic parameters\n",
    "    N,DOF1 = X.size();\n",
    "    DOF2 = Y.size()[1];\n",
    "    N_val = X_val.size()[0]\n",
    "    lip = findLipschitzConstant(X,Y)\n",
    "    L = torch.exp(torch.linspace(0,torch.log(lip),k_fold))\n",
    "    StartTime = time.time()\n",
    "    \n",
    "    #important values\n",
    "    \n",
    "    no_improvement  = 0\n",
    "    lip_const  = inf #the returned lipschitz constant of the CV process\n",
    "    Phi = inf # the returned score score sum(||Y_i-Z_i||)/N , \n",
    "    Z_train = torch.zeros(Y.size()), #the returned smoothing\n",
    "    \n",
    "#    if RAD: #if you choose use Rademacher complexity generalization bounds\n",
    "#        stop_cond_rad  = True\n",
    "#        lip_const_rad = inf\n",
    "#        rad = inf\n",
    "#        Z_train_rad = torch.zeros(Y.size())\n",
    "        \n",
    "    for l in L:\n",
    "        tmp_time = time.time()\n",
    "        #Z_train_l,rad_l = FindSolutionAndReport(Y,X,l,False) # smoothig by l\n",
    "        Z_train = FindSolutionAndReport(Y,X,l,False) # smoothig by l        \n",
    "        Z_val = Test(X_val,X,Z_train,eps,l) # validating\n",
    "#debug        print(\"debug2\",Z_val)\n",
    "        Phi_l = lstsqrs(Z_val,Y_val) # l_smoothing scoretorch.sum((Z_val_l - Y_val) ** 2).item() #\n",
    "        #print(\"debug3\", l,Phi_l, CV)\n",
    "        if Phi_l < Phi:\n",
    "#debug            print(\"new phi:\", Phi_l)\n",
    "            Phi = Phi_l\n",
    "            lip_const = l\n",
    "            #Z_train = Z_train_l\n",
    "            no_improvement = 0\n",
    "            \n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            \n",
    "        if no_improvement >= 2: # if it is the second time it means we start over fitting so we stop\n",
    "            print(\"lip \",l,\" Phi = \",Phi_l, \"time = \", round(time.time()- tmp_time,2), \"sec\")\n",
    "            print(\"We start overfitting the data\")\n",
    "            break\n",
    "        stop_cond = True\n",
    "        print(\"lip \",l,\" Phi = \",Phi_l, \"time = \", round(time.time()- tmp_time,2), \"sec\")\n",
    "    print(\"finished training \",N,\" points \",k_fold,\"crossValidation in \",round(time.time()- StartTime,2),\" sec\")\n",
    "    print(\"Lip_const_cv: l=\", lip_const,\" with score: Phi=\",Phi)\n",
    "#    if RAD: print(\"Lip_const_rad:l=\", lip_const_rad,\" with generalization bound: \",rad)\n",
    "    \n",
    "    return Z_train,lip_const\n",
    "\n",
    "#Z_train,lip_const = crossVal(X_train,Y_train,X_val,Y_val,k_fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXdGG1wkISln"
   },
   "source": [
    "For Testing our model, we first import the data, which should be allocated in a directory named data - placed in the same directory as this code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to learn py positioning (may lead to distortions)\n",
    "N_train = 10000\n",
    "N_test = 1000\n",
    "data_dir = 'data_3to5_positions'\n",
    "X = torch.Tensor(np.array(pd.read_csv(data_dir+'\\posX_train.csv', header = None))[:N_train,:]).to(device)\n",
    "Y = torch.Tensor(np.array(pd.read_csv(data_dir+'\\posY_train.csv', header = None))[:N_train,:]).to(device)\n",
    "#noise = torch.randn(Y.size()) * Y.std()/10\n",
    "#Y = Y + noise\n",
    "X_test = torch.Tensor(np.array(pd.read_csv(data_dir+'\\posX_test.csv', header = None))[:N_test,:]).to(device)\n",
    "Y_test = torch.Tensor(np.array(pd.read_csv(data_dir+'\\posY_test.csv', header = None))[:N_test,:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: this temporary box should be integrated in the cross validation function\n",
    "from random import sample\n",
    "train_size = N_train*9//10\n",
    "R = sample(range(N_train),train_size+train_size//10)\n",
    "I,J = R[:train_size],R[train_size:]\n",
    "X_train,X_val = X[I],X[J]\n",
    "Y_train,Y_val = Y[I],Y[J]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61.4043)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findLipschitzConstant(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For thousands of points running time of FindAndReport could be in hours. therefor we find the Lipschitz constant via Cross valditaion over only part of the training set. Once we picked the lip_const of the problem, we can train smooth the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<><><><"
     ]
    }
   ],
   "source": [
    "#CV_N = 1000\n",
    "Z_train,lip_const = crossVal(X_train,Y_train,X_val,Y_val,k_fold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "H8xPluo5ISl7",
    "outputId": "2041c077-bfd6-46bc-8225-de4da500042d"
   },
   "outputs": [],
   "source": [
    "Z_train =  FindSolutionAndReport(Y_train,X_train,lip_const,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code if you already run the algorithm and\\or want to save time for the exploration part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_train = torch.Tensor(np.array(pd.read_csv(data_dir+'\\Z_train.csv', header = None))[:N_train,:]).to(device)\n",
    "#Z_test = torch.Tensor(np.array(pd.read_csv(data_dir+'\\Z_test_mw.csv', header = None))[:N_test,:]).to(device)\n",
    "#lip_const = findLipschitzConstant(X_train,Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SuvgqVlPISmZ",
    "outputId": "ce1cfd32-49ef-4514-90c3-ddeda7d95947"
   },
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "Z_test = Test(X_test,X_train,Z_train,eps,lip_const)\n",
    "print(\"Test \",Z_test.size()[0],\"samples in \",  round(time.time()- startTime,2),\" sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1CAVreoZISms",
    "outputId": "b8d65f39-c27a-40b2-bca4-b47f570c1cdc"
   },
   "outputs": [],
   "source": [
    "print(\"AVG lstsqrs: \",lstsqrs(Z_test,Y_test)/N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot4Arms(arm1,arm2,arm3,arm4):\n",
    "    x1,y1 = arm1[0::2],arm1[1::2]\n",
    "    x2,y2 = arm2[0::2],arm2[1::2]\n",
    "    x3,y3 = arm3[0::2],arm3[1::2]\n",
    "    x4,y4 = arm4[0::2],arm4[1::2]\n",
    "    plt.plot(x1,y1,'k-',x2,y2,'y-',x3,y3,'b-',x4,y4,'r')\n",
    "    \n",
    "def plot3Arms(arm1,arm2,arm3):\n",
    "    x1,y1 = arm1[0::2],arm1[1::2]\n",
    "    x2,y2 = arm2[0::2],arm2[1::2]\n",
    "    x3,y3 = arm3[0::2],arm3[1::2]\n",
    "    plt.plot(x1,y1,linestyle = '--',marker='o',color = 'r') #true\n",
    "    plt.plot(x2,y2,linestyle = '-',marker='o',color = 'y') #competetor\n",
    "    plt.plot(x3,y3,linestyle = '-',marker='o',color = 'b') #MWU\n",
    "\n",
    "def plot2Arms(arm1,arm2):\n",
    "    x1,y1 = arm1[0::2],arm1[1::2]\n",
    "    x2,y2 = arm2[0::2],arm2[1::2]\n",
    "    plt.plot(x1,y1,linestyle = '--',marker='o',color = 'r') #true\n",
    "    plt.plot(x2,y2,linestyle = '-',marker='o',color = 'b') #learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the MW learner final positions (where it matters), ploting the learned correspondence.\n",
    "We will plot 2 arms on the same figure:\n",
    "1. Red\\dashed = y. The true, *unknown* correspondence of the learner. It can easily spotted the the end frame of the learner and expert should always coinside. 3 degrees of freedom\n",
    "2. Blue = z_mw The learned correspondence by the MWU learner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "I = [5,15,19,28]# sample(range(N_test),20); # change for as much figures you want to plot\n",
    "for i in I:\n",
    "    print(i,\":\")\n",
    "    plot2Arms(Y_test[i].numpy(),Z_test[i].numpy())\n",
    "    ax = plt.gca()\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MW_Kirszbraun_Algorithm - GPU.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
